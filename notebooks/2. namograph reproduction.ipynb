{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a35fe03a-ffbd-4da9-a4aa-c97766b3d0dc",
   "metadata": {},
   "source": [
    "<font size=\"3\">Here I reproduce the data transformation and machine learning parts of Antoine Mazières and Camille Roth's paper *'Large-scale diversity estimation through surname origin inference'* : https://namograph.antonomase.fr/ .\n",
    "First doing the same thing on the same data without reusing their code and trying a different model for the sake of fun.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55b6240d-7599-42ae-986a-4682a8c0c19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import punctuation, ascii_letters\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, plot_confusion_matrix\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import pickle\n",
    "#dictionnary mapping non ascii to ascii characters\n",
    "caract_mapping = pd.read_html('https://docs.oracle.com/cd/E29584_01/webhelp/mdex_basicDev/src/rbdv_chars_mapping.html')\n",
    "carac_dict = pd.DataFrame(np.concatenate([caract_mapping[i].iloc[:,[1,2]].values for i in (0,1)])).set_index(0).to_dict('dict')[1]\n",
    "\n",
    "def name_cleaner(name):\n",
    "    clean = \"\"\n",
    "    for letter in name:\n",
    "        if (letter.lower() in ascii_letters) or (letter.lower() in \"\"\" ''`'-  \"\"\"):\n",
    "            clean += letter\n",
    "        elif letter in punctuation:\n",
    "            return 'ERROR'\n",
    "        elif letter in carac_dict.keys():\n",
    "            clean += carac_dict[letter]\n",
    "        else:\n",
    "            return 'ERROR'\n",
    "            \n",
    "    return clean.upper()\n",
    "\n",
    "#reusing exactly the same clusters\n",
    "clusters = {\n",
    "    \"African\": [\"Zimbabwe\",\"Rwanda\",\"Zambia\",\"Malawi\",\"Tanzania\",\"Uganda\",\"Kenya\",\"Dem. Rep. Congo\",\"Congo\",\"South Africa\",\"Gambia\",\"Botswana\",\"Mozambique\",\"Mali\",\"Trinidad and Tobago\",\"Gabon\",\"Cameroon\",\"Benin\",\"Côte d'Ivoire\",\"Burkina Faso\",\"Togo\",\"Senegal\",\"Nigeria\",\"Ghana\",\"Ethiopia\"],\n",
    "    \"Asian\": [\"Vietnam\",\"China\",\"Thailand\",\"Cambodia\",\"Taiwan\",\"Korea\",\"Lao PDR\",\"Japan\",\"Indonesia\",\"Philippines\"],\n",
    "    \"Indian\": [\"Nepal\",\"India\",\"Sri Lanka\",\"Mongolia\",\"Pakistan\",\"Malaysia\",\"Bangladesh\",\"Iran\"],\n",
    "    \"Arabian\": [\"Sudan\",\"Libya\",\"Egypt\",\"Tunisia\",\"Morocco\",\"Algeria\",\"United Arab Emirates\",\"Qatar\",\"Lebanon\",\"Syria\",\"Jordan\",\"Palestine\",\"Saudi Arabia\",\"Kuwait\",\"Iraq\",\"Oman\",\"Yemen\"],\n",
    "    \"Slavic\": [\"Poland\",\"Macedonia\",\"Ukraine\",\"Belarus\",\"Russia\",\"Kazakhstan\",\"Bulgaria\",\"Slovakia\",\"Czech Rep.\",\"Croatia\",\"Bosnia and Herz.\",\"Serbia\",\"Montenegro\"],\n",
    "    \"NorthEuropean\": [\"Norway\",\"Jamaica\",\"Denmark\",\"Sweden\",\"Netherlands\",\"Belgium\",\"Germany\",\"Austria\",\"United Kingdom\",\"Australia\",\"Canada\",\"New Zealand\",\"United States\",\"Ireland\",\"Israel\",\"Switzerland\",\"Luxembourg\",\"France\",\"Iceland\"],\n",
    "    \"CentralSouthEuropean\": [\"Slovenia\",\"Hungary\",\"Turkey\",\"Latvia\",\"Estonia\",\"Finland\",\"Italy\",\"Albania\",\"Romania\",\"Lithuania\",\"Greece\",\"Cyprus\",\"Georgia\",\"Venezuela\",\"Puerto Rico\",\"Costa Rica\",\"Spain\",\"Mexico\",\"Cuba\",\"Colombia\",\"Guatemala\",\"Peru\",\"Chile\",\"Ecuador\",\"Bolivia\",\"Uruguay\",\"Argentina\",\"Panama\",\"Portugal\",\"Brazil\"],\n",
    "}\n",
    "\n",
    "region_dict = pd.Series(clusters).explode().reset_index().iloc[:,[1,0]].set_index(0).to_dict(\"dict\")[\"index\"]\n",
    "\n",
    "def print_metrics(y_test,y_pred):\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    results = df.sort_values(\"f1-score\",ascending=False)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0663a6-a42f-40f1-b542-fd1560c58414",
   "metadata": {},
   "source": [
    "# Creating the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8cf6e2-75ab-4688-8b78-a81007fa8a09",
   "metadata": {},
   "source": [
    "## loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19055110-3ba4-440a-9fe1-39dff858194e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230 incorrect names removed, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-ab771ad7b2e1>:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_arabic[\"name\"] = df_arabic['name'].str.replace(\"-\",\" \").str.strip()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabic names split, dashes handled, Chunk processed, total rows processed :  2073455\n",
      "272 incorrect names removed, arabic names split, dashes handled, Chunk processed, total rows processed :  4145818\n",
      "228 incorrect names removed, arabic names split, dashes handled, Chunk processed, total rows processed :  6217048\n",
      "214 incorrect names removed, arabic names split, dashes handled, Chunk processed, total rows processed :  8289071\n",
      "254 incorrect names removed, arabic names split, dashes handled, Chunk processed, total rows processed :  10360287\n",
      "234 incorrect names removed, arabic names split, dashes handled, Chunk processed, total rows processed :  12431940\n",
      "223 incorrect names removed, arabic names split, dashes handled, Chunk processed, total rows processed :  14503179\n",
      "229 incorrect names removed, arabic names split, dashes handled, Chunk processed, total rows processed :  16577115\n",
      "195 incorrect names removed, arabic names split, dashes handled, Chunk processed, total rows processed :  18649084\n",
      "163 incorrect names removed, arabic names split, dashes handled, Chunk processed, total rows processed :  20716477\n",
      "110 incorrect names removed, arabic names split, dashes handled, Chunk processed, total rows processed :  22778172\n",
      "43 incorrect names removed, arabic names split, dashes handled, Chunk processed, total rows processed :  24834395\n",
      "17 incorrect names removed, arabic names split, dashes handled, Chunk processed, total rows processed :  26503626\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>name</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AAMOON</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>ABDUL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>ADEGBOYE</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>ADEL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AHADI</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913037</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>ZISHIRI</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913038</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>ZVANDASARA</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913039</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>ZVAUYA</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913040</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>ZVINAVASHE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913041</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>ZVINOROVA</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1913042 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             country        name  count\n",
       "0        Afghanistan      AAMOON      1\n",
       "1        Afghanistan       ABDUL      1\n",
       "2        Afghanistan    ADEGBOYE      2\n",
       "3        Afghanistan        ADEL      1\n",
       "4        Afghanistan       AHADI      1\n",
       "...              ...         ...    ...\n",
       "1913037     Zimbabwe     ZISHIRI      3\n",
       "1913038     Zimbabwe  ZVANDASARA      9\n",
       "1913039     Zimbabwe      ZVAUYA      2\n",
       "1913040     Zimbabwe  ZVINAVASHE      1\n",
       "1913041     Zimbabwe   ZVINOROVA      6\n",
       "\n",
       "[1913042 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load and clean the dataframe\n",
    "\n",
    "file_path = \"../data/other_data/pubmed_name_country.csv\"\n",
    "length = 0\n",
    "df_list = []\n",
    "chunksize=2000000\n",
    "\n",
    "#processing the csv in chunks because it's too heavy\n",
    "for chunk in pd.read_csv(file_path,sep=\";\",header=None, dtype={0 : 'string',1 : \"category\"},chunksize=chunksize):\n",
    "    df = chunk\n",
    "    df.columns = [\"name\",\"country\"]\n",
    "    len1 = len(df)\n",
    "    \n",
    "    #cleaning the name\n",
    "    df[\"name\"] = df[\"name\"].fillna(\"ERROR\").apply(name_cleaner).str.strip() \n",
    "    df = df[df[\"name\"] != \"ERROR\"]\n",
    "    len2 = len(df)\n",
    "    print(str(len1 - len2) + \" incorrect names removed\", end=\", \")\n",
    "    \n",
    "    #handling arabic names starting with EL/AL\n",
    "    df_arabic = df[df[\"name\"].str[:3].str.contains(\"^[AE]L[- ]\")]  \n",
    "    df_arabic[\"name\"] = df_arabic['name'].str.replace(\"-\",\" \").str.strip()\n",
    "    print(\"arabic names split\", end=\", \")\n",
    "\n",
    "    df = df[~df.index.isin(df_arabic.index)]\n",
    "    \n",
    "    #splitting names with dashes into multiple names\n",
    "    df[\"name\"] = df[\"name\"].str.split(\"-\") \n",
    "    df = df.explode(\"name\")\n",
    "    df = pd.concat([df_arabic,df])\n",
    "    print(\"dashes handled\", end=\", \")\n",
    "\n",
    "    df= df[~df[\"name\"].str.contains(\"^[A-Z] [A-Z]$\")] #removing one letter space one letter names\n",
    "    df = df[(df[\"name\"].str.len() > 1) & (df[\"name\"].str.len() <=30)] #removing too short and too long names\n",
    "    df_list.append(df)\n",
    "    \n",
    "    length +=len(df)\n",
    "    print(\"Chunk processed, total rows processed : \", length)\n",
    "df = pd.concat(df_list)\n",
    "del df_list\n",
    "df = df.groupby([\"country\",\"name\"])[\"name\"].count().rename(\"count\").reset_index()\n",
    "df\n",
    "#ps : the paper's code says that names present less than 1000 times are removed but that's not the case. The dataframe would be 68 000 rows long otherwise!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6432fb8-33fd-4721-935b-ef36a53e6da2",
   "metadata": {},
   "source": [
    "## Cleaning the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee47134-aa41-4ee7-9176-1c646981b947",
   "metadata": {},
   "source": [
    "As explained in the paper, we assume that names that highly cluster in one country originate from this country. We use the Herfindahl-Hirschman Index to check this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d250c0b-5706-49cc-8f0a-93c85327a51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>name</th>\n",
       "      <th>count</th>\n",
       "      <th>freq</th>\n",
       "      <th>hhi</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1438045</th>\n",
       "      <td>Uganda</td>\n",
       "      <td>ZZIWA</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>African</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965870</th>\n",
       "      <td>Malaysia</td>\n",
       "      <td>ZZ</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124428</th>\n",
       "      <td>Poland</td>\n",
       "      <td>ZYZYNSKA</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Slavic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088809</th>\n",
       "      <td>Panama</td>\n",
       "      <td>ZYZNIEUSKI</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000367</td>\n",
       "      <td>0.998398</td>\n",
       "      <td>CentralSouthEuropean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124427</th>\n",
       "      <td>Poland</td>\n",
       "      <td>ZYZELEWICZ</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Slavic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218130</th>\n",
       "      <td>Spain</td>\n",
       "      <td>DUENAS</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>CentralSouthEuropean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218129</th>\n",
       "      <td>Spain</td>\n",
       "      <td>CEBALLOS</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>CentralSouthEuropean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096665</th>\n",
       "      <td>Poland</td>\n",
       "      <td>BLASZCZYK</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Slavic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791287</th>\n",
       "      <td>Israel</td>\n",
       "      <td>BEN</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NorthEuropean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218128</th>\n",
       "      <td>Spain</td>\n",
       "      <td>ASEGUINOLAZA</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>CentralSouthEuropean</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>650566 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          country           name  count      freq       hhi  \\\n",
       "1438045    Uganda          ZZIWA      1  0.000087  1.000000   \n",
       "965870   Malaysia             ZZ      1  0.000015  1.000000   \n",
       "1124428    Poland       ZYZYNSKA      3  0.000011  1.000000   \n",
       "1088809    Panama     ZYZNIEUSKI      1  0.000367  0.998398   \n",
       "1124427    Poland     ZYZELEWICZ      2  0.000008  1.000000   \n",
       "...           ...            ...    ...       ...       ...   \n",
       "1218130     Spain         DUENAS      1  0.000001  1.000000   \n",
       "1218129     Spain       CEBALLOS      1  0.000001  1.000000   \n",
       "1096665    Poland      BLASZCZYK      1  0.000004  1.000000   \n",
       "791287     Israel            BEN      2  0.000009  1.000000   \n",
       "1218128     Spain   ASEGUINOLAZA      1  0.000001  1.000000   \n",
       "\n",
       "                       region  \n",
       "1438045               African  \n",
       "965870                 Indian  \n",
       "1124428                Slavic  \n",
       "1088809  CentralSouthEuropean  \n",
       "1124427                Slavic  \n",
       "...                       ...  \n",
       "1218130  CentralSouthEuropean  \n",
       "1218129  CentralSouthEuropean  \n",
       "1096665                Slavic  \n",
       "791287          NorthEuropean  \n",
       "1218128  CentralSouthEuropean  \n",
       "\n",
       "[650566 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data  = df.copy()\n",
    "\n",
    "#computing country level frequency\n",
    "data[\"freq\"] = data[\"count\"]/data.groupby(\"country\")[\"count\"].transform(sum)\n",
    "\n",
    "#computing normalized frequency\n",
    "standardized_freq = data[\"freq\"]/data.groupby(\"name\")[\"freq\"].transform(sum)\n",
    "\n",
    "#computing the hhi\n",
    "data[\"hhi\"] = (standardized_freq**2).groupby(data['name']).transform(sum)\n",
    "\n",
    "data = data[(data[\"hhi\"] > 0.8) & (data['freq'] > 0.000001)]\n",
    "data = data.sort_values(['name','freq'],ascending=False).drop_duplicates(\"name\")\n",
    "\n",
    "data[\"region\"] = data[\"country\"].apply(lambda x : region_dict[x] if x in region_dict.keys() else None)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df9ec1a-d439-4cd5-8c26-4d34261d0f74",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15420929-fb6f-40a4-9bd6-6cf21394e6a7",
   "metadata": {},
   "source": [
    "## Reproducing the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b243eb43-e2e0-4de7-bbde-bd35fad96062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CentralSouthEuropean</th>\n",
       "      <td>0.807520</td>\n",
       "      <td>0.715045</td>\n",
       "      <td>0.758474</td>\n",
       "      <td>28415.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Slavic</th>\n",
       "      <td>0.634583</td>\n",
       "      <td>0.835229</td>\n",
       "      <td>0.721211</td>\n",
       "      <td>9583.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.716220</td>\n",
       "      <td>0.695010</td>\n",
       "      <td>0.698376</td>\n",
       "      <td>96590.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.695010</td>\n",
       "      <td>0.695010</td>\n",
       "      <td>0.695010</td>\n",
       "      <td>0.69501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NorthEuropean</th>\n",
       "      <td>0.774949</td>\n",
       "      <td>0.623629</td>\n",
       "      <td>0.691103</td>\n",
       "      <td>32561.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asian</th>\n",
       "      <td>0.610533</td>\n",
       "      <td>0.766180</td>\n",
       "      <td>0.679558</td>\n",
       "      <td>6582.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indian</th>\n",
       "      <td>0.633328</td>\n",
       "      <td>0.716803</td>\n",
       "      <td>0.672485</td>\n",
       "      <td>10159.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.631620</td>\n",
       "      <td>0.712179</td>\n",
       "      <td>0.662521</td>\n",
       "      <td>96590.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arabian</th>\n",
       "      <td>0.523134</td>\n",
       "      <td>0.719406</td>\n",
       "      <td>0.605768</td>\n",
       "      <td>4715.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>African</th>\n",
       "      <td>0.437294</td>\n",
       "      <td>0.608962</td>\n",
       "      <td>0.509044</td>\n",
       "      <td>4575.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      precision    recall  f1-score      support\n",
       "CentralSouthEuropean   0.807520  0.715045  0.758474  28415.00000\n",
       "Slavic                 0.634583  0.835229  0.721211   9583.00000\n",
       "weighted avg           0.716220  0.695010  0.698376  96590.00000\n",
       "accuracy               0.695010  0.695010  0.695010      0.69501\n",
       "NorthEuropean          0.774949  0.623629  0.691103  32561.00000\n",
       "Asian                  0.610533  0.766180  0.679558   6582.00000\n",
       "Indian                 0.633328  0.716803  0.672485  10159.00000\n",
       "macro avg              0.631620  0.712179  0.662521  96590.00000\n",
       "Arabian                0.523134  0.719406  0.605768   4715.00000\n",
       "African                0.437294  0.608962  0.509044   4575.00000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#traing the model\n",
    "\n",
    "def padding(name):\n",
    "    padded_text = \"^\" + name + \"$\"\n",
    "    return padded_text\n",
    "\n",
    "padder = FunctionTransformer(padding)\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(1, 4))\n",
    "NB_model = MultinomialNB(alpha=0.01, fit_prior=True)\n",
    "\n",
    "\n",
    "clf = Pipeline(steps=[(\"padding\",padder),\n",
    "                      ('vectorizer', vectorizer),\n",
    "                      ('model', NB_model)])\n",
    "\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "X = data[\"name\"]\n",
    "y = data[\"region\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print_metrics(y_test,y_pred)\n",
    "#checks out!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff602fc-e9ac-44c9-acf1-ac4ba8a2ef27",
   "metadata": {},
   "source": [
    "## Improving performance\n",
    "\n",
    "Let's start by switching to a SGDClassifier with a wider ngram range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e864e044-5852-4b83-b91b-05784f12a1f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CentralSouthEuropean</th>\n",
       "      <td>0.831892</td>\n",
       "      <td>0.788914</td>\n",
       "      <td>0.809833</td>\n",
       "      <td>28415.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asian</th>\n",
       "      <td>0.716462</td>\n",
       "      <td>0.807353</td>\n",
       "      <td>0.759197</td>\n",
       "      <td>6582.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Slavic</th>\n",
       "      <td>0.674447</td>\n",
       "      <td>0.865387</td>\n",
       "      <td>0.758079</td>\n",
       "      <td>9583.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.761115</td>\n",
       "      <td>0.743038</td>\n",
       "      <td>0.745280</td>\n",
       "      <td>96590.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indian</th>\n",
       "      <td>0.703357</td>\n",
       "      <td>0.791909</td>\n",
       "      <td>0.745011</td>\n",
       "      <td>10159.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.743038</td>\n",
       "      <td>0.743038</td>\n",
       "      <td>0.743038</td>\n",
       "      <td>0.743038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NorthEuropean</th>\n",
       "      <td>0.822631</td>\n",
       "      <td>0.648383</td>\n",
       "      <td>0.725187</td>\n",
       "      <td>32561.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.682047</td>\n",
       "      <td>0.759845</td>\n",
       "      <td>0.712448</td>\n",
       "      <td>96590.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arabian</th>\n",
       "      <td>0.542563</td>\n",
       "      <td>0.759703</td>\n",
       "      <td>0.633030</td>\n",
       "      <td>4715.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>African</th>\n",
       "      <td>0.482975</td>\n",
       "      <td>0.657268</td>\n",
       "      <td>0.556800</td>\n",
       "      <td>4575.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      precision    recall  f1-score       support\n",
       "CentralSouthEuropean   0.831892  0.788914  0.809833  28415.000000\n",
       "Asian                  0.716462  0.807353  0.759197   6582.000000\n",
       "Slavic                 0.674447  0.865387  0.758079   9583.000000\n",
       "weighted avg           0.761115  0.743038  0.745280  96590.000000\n",
       "Indian                 0.703357  0.791909  0.745011  10159.000000\n",
       "accuracy               0.743038  0.743038  0.743038      0.743038\n",
       "NorthEuropean          0.822631  0.648383  0.725187  32561.000000\n",
       "macro avg              0.682047  0.759845  0.712448  96590.000000\n",
       "Arabian                0.542563  0.759703  0.633030   4715.000000\n",
       "African                0.482975  0.657268  0.556800   4575.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.dropna()\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=\"char_wb\", ngram_range=(2,8))\n",
    "SGD_model = SGDClassifier(class_weight=\"balanced\",max_iter=1000)\n",
    "\n",
    "clf = Pipeline(steps=[(\"padding\",padder),\n",
    "                      ('vectorizer', vectorizer),\n",
    "                      ('model', SGD_model)])\n",
    "\n",
    "X = data[\"name\"]\n",
    "y = data[\"region\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print_metrics(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f163df-bc1f-4189-a4ad-803722317307",
   "metadata": {},
   "source": [
    "Then by training the model on a dataset where the United States, Canada and Switzerland have removed before computing the hhi because they are too heterogenous and are confusing the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4743a07d-1e73-468c-b263-c3bcc1646924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CentralSouthEuropean</th>\n",
       "      <td>0.829712</td>\n",
       "      <td>0.790146</td>\n",
       "      <td>0.809446</td>\n",
       "      <td>28415.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Slavic</th>\n",
       "      <td>0.680053</td>\n",
       "      <td>0.860586</td>\n",
       "      <td>0.759742</td>\n",
       "      <td>9583.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asian</th>\n",
       "      <td>0.683072</td>\n",
       "      <td>0.820267</td>\n",
       "      <td>0.745409</td>\n",
       "      <td>6582.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.761030</td>\n",
       "      <td>0.740284</td>\n",
       "      <td>0.742720</td>\n",
       "      <td>96590.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indian</th>\n",
       "      <td>0.696878</td>\n",
       "      <td>0.790924</td>\n",
       "      <td>0.740929</td>\n",
       "      <td>10159.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.740284</td>\n",
       "      <td>0.740284</td>\n",
       "      <td>0.740284</td>\n",
       "      <td>0.740284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NorthEuropean</th>\n",
       "      <td>0.833059</td>\n",
       "      <td>0.637849</td>\n",
       "      <td>0.722501</td>\n",
       "      <td>32561.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.676619</td>\n",
       "      <td>0.759977</td>\n",
       "      <td>0.708673</td>\n",
       "      <td>96590.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arabian</th>\n",
       "      <td>0.548877</td>\n",
       "      <td>0.751432</td>\n",
       "      <td>0.634378</td>\n",
       "      <td>4715.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>African</th>\n",
       "      <td>0.464682</td>\n",
       "      <td>0.668634</td>\n",
       "      <td>0.548306</td>\n",
       "      <td>4575.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      precision    recall  f1-score       support\n",
       "CentralSouthEuropean   0.829712  0.790146  0.809446  28415.000000\n",
       "Slavic                 0.680053  0.860586  0.759742   9583.000000\n",
       "Asian                  0.683072  0.820267  0.745409   6582.000000\n",
       "weighted avg           0.761030  0.740284  0.742720  96590.000000\n",
       "Indian                 0.696878  0.790924  0.740929  10159.000000\n",
       "accuracy               0.740284  0.740284  0.740284      0.740284\n",
       "NorthEuropean          0.833059  0.637849  0.722501  32561.000000\n",
       "macro avg              0.676619  0.759977  0.708673  96590.000000\n",
       "Arabian                0.548877  0.751432  0.634378   4715.000000\n",
       "African                0.464682  0.668634  0.548306   4575.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I've simply rerun the above step of cleaning data without those countries. Not shown here.\n",
    "data = data.dropna()\n",
    "\n",
    "X = data[\"name\"]\n",
    "y = data[\"region\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print_metrics(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1300ec2-df35-49d4-8ec9-c5d279dce064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MR_model.joblib']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saving the model\n",
    "from joblib import dump\n",
    "dump(clf, 'MR_model.joblib') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759e5a0a-c395-45aa-95e5-33cec9a880bb",
   "metadata": {},
   "source": [
    "That's all !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_data",
   "language": "python",
   "name": "base_data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc-autonumbering": true,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
