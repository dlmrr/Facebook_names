{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f4a6e04-acbc-454d-8bf1-f5bf74c89a7f",
   "metadata": {},
   "source": [
    "<font size=\"3\"> I show how to create surnames and first names datasets from the data of the Facebook leak of 2021. It covers about 100 countries. Not putting the link here but it can be found easily in torrent format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd8ae77-57a5-4ba9-8077-3f67b313777c",
   "metadata": {},
   "source": [
    "# Extracting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb8d0b7-1b8d-4ded-aca7-8e80d9df02b3",
   "metadata": {},
   "source": [
    "The data takes the form of multiple compressed files, one for each country. The compression format is either zip, rar or 7zip. The files are in csv format with either a comma or colon separator. Sometimes the below function didn't work because the file was too big or invalid or the format was different. In this case I took a custom approach not detailed here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74a8a886-d7bd-4acf-88de-63ac9c4dfed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import io\n",
    "import re\n",
    "import zipfile\n",
    "import rarfile\n",
    "import os \n",
    "from string import punctuation, ascii_letters\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_sep(file): #The separator is sometimes a comma and sometimes a colon. This function finds the kind of separator of the file.\n",
    "    head_a = str(file[:5000])\n",
    "    comma_count = head_a.count(\",\")\n",
    "    colon_count = head_a.count(\":\")\n",
    "    if comma_count > colon_count :\n",
    "        sep = \",\"\n",
    "    else:\n",
    "        sep=\":\"\n",
    "    return sep\n",
    "\n",
    "\n",
    "def df_from_archive(archive):\n",
    "    df_list = []\n",
    "    files = archive.namelist()  #the data is usually split into multiple files inside the archive\n",
    "    \n",
    "    for file in files : \n",
    "        print(file)\n",
    "        \n",
    "        with archive.open(file) as myfile:\n",
    "            a = myfile.read()\n",
    "            sep = get_sep(a)\n",
    "\n",
    "            try:\n",
    "                chunks_list = [] #loading files in chunks to avoid crashing               \n",
    "                for chunk in pd.read_table(io.StringIO(a.decode(\"utf-8\")),sep=sep,header=None,usecols=[2,3,4], quoting=3,engine=\"c\",dtype=pd.StringDtype(),chunksize=1000000):\n",
    "                    chunks_list.append(chunk)\n",
    "                df = pd.concat(chunks_list)\n",
    "                \n",
    "            except:\n",
    "                chunks_list = []\n",
    "                print(\"failure,switching to python engine\")  #sometimes the c engine will crash, in that case the python engine can sometimes do the trick\n",
    "                for chunk in pd.read_table(io.StringIO(a.decode(\"utf-8\")),sep=sep,header=None,usecols=[2,3,4], quoting=3,engine=\"python\",dtype=pd.StringDtype(),chunksize=1000000):\n",
    "                    chunks_list.append(chunk)\n",
    "                df = pd.concat(chunks_list)\n",
    "\n",
    "            df.columns = [\"first_name\",\"surname\",\"gender\"]\n",
    "            df_list.append(df)\n",
    "            \n",
    "    df = pd.concat(df_list)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_df(compressed_file):\n",
    "    try:\n",
    "        if \".zip\" in compressed_file:\n",
    "            with zipfile.ZipFile(compressed_file, 'r') as archive:\n",
    "                df = df_from_archive(archive)\n",
    "\n",
    "        elif \".rar\" in compressed_file:\n",
    "            with rarfile.RarFile(compressed_file, 'r') as archive: \n",
    "                df = df_from_archive(archive)\n",
    "    except:\n",
    "        print(\"File error : \", compressed_file)  \n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4d8004-64e0-4eb7-af59-0dbfe450dbe6",
   "metadata": {},
   "source": [
    "I create two dataframes for each country, one for first names, one for surnames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e348fbe-0e17-4232-bf89-709d6e0973d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(\"facebook/*.rar\") + glob(\"facebook/*.zip\")\n",
    "print(len(files), \" countries\")\n",
    "\n",
    "for directory in [\"surnames\", \"first_names\"]:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "for n,file in enumerate(files) :\n",
    "    country = file.replace(\"facebook/\",'').replace(\".zip\",\"\").replace(\".rar\",\"\").lower()\n",
    "    print('nÂ°' + str(n+1) +\" : \" + country)\n",
    "    \n",
    "    df = get_df(file)\n",
    "    \n",
    "    surnames = df[\"surname\"].value_counts().rename(\"count\")\n",
    "    print(\"len surnames : \", len(surnames))\n",
    "    surnames.to_csv(f'surnames/{country}_surnames.csv')\n",
    "    \n",
    "    #also extracting a first name dataset just in case\n",
    "    first_names = df[[\"first_name\",\"gender\"]].value_counts().rename(\"count\")\n",
    "    print(\"len first names : \", len(first_names))\n",
    "    first_names.to_csv(f'first_names/{country}_first_names.csv')\n",
    "               \n",
    "    print(country, \" done\",end=\"\\n\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fb_names",
   "language": "python",
   "name": "fb_names"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
