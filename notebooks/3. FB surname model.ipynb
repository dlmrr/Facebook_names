{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f3671bc-955a-4fdc-b712-6cc58e7e8991",
   "metadata": {},
   "source": [
    "<font size=\"3\">Now I train the previous model on the Facebook data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fd1db9d-a6ec-4d63-b28c-28cbd28148e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import io\n",
    "import re\n",
    "import zipfile\n",
    "import rarfile\n",
    "import os \n",
    "from string import punctuation, ascii_letters\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, plot_confusion_matrix, cohen_kappa_score, balanced_accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from joblib import dump, load\n",
    "\n",
    "\n",
    "#dictionnary mapping non ascii to ascii characters\n",
    "caract_mapping = pd.read_html('https://docs.oracle.com/cd/E29584_01/webhelp/mdex_basicDev/src/rbdv_chars_mapping.html')\n",
    "carac_dict = pd.DataFrame(np.concatenate([caract_mapping[i].iloc[:,[1,2]].values for i in (0,1)])).set_index(0).to_dict('dict')[1]\n",
    "\n",
    "def name_cleaner(name):\n",
    "    clean = \"\"\n",
    "    for letter in name:\n",
    "        if (letter.lower() in ascii_letters) or (letter.lower() in \"\"\" ''`'-  \"\"\"):\n",
    "            clean += letter\n",
    "        elif letter in punctuation:\n",
    "            return 'error'\n",
    "        elif letter in carac_dict.keys():\n",
    "            clean += carac_dict[letter]\n",
    "        else:\n",
    "            return 'error'\n",
    "            \n",
    "    return clean.upper()\n",
    "\n",
    "\n",
    "def islatin(string):\n",
    "    try:\n",
    "        a = re.search(\"[A-Za-z]+\",string.replace(\" \",\"\")).group(0)\n",
    "        if len(a) == len(string.replace(\" \",\"\")):\n",
    "            isok= 1\n",
    "        else:\n",
    "            isok= 2\n",
    "    except:\n",
    "        isok = 0\n",
    "    return isok\n",
    "\n",
    "def print_metrics(y_test,y_pred):\n",
    "    bas = balanced_accuracy_score(y_test,y_pred)\n",
    "    cks = cohen_kappa_score(y_test,y_pred)\n",
    "    print(\"Balanced accuracy score : \", round(bas,3))\n",
    "    print(\"Cohen Kappa score : \", round(cks, 3))\n",
    "\n",
    "\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    results = df.sort_values(\"f1-score\",ascending=False)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d08f1f4-3b42-4702-8b3a-3b9444f61be9",
   "metadata": {},
   "source": [
    "# Building the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "aafbf007-af14-45e4-bda6-cbb574b4d23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Afghanistan : 4475, Albania : 9064, Algeria : 100610, Angola : 7069, Argentina : 28039, Austria : 27468, Azerbaijan : 1387, Bahrain : 16995, Bangladesh : 21074, Belgium : 61747, Bolivia : 32655, Botswana : 6439, Brazil : 55988, Brunei : 4108, Bulgaria : 4226, Burkina Faso : 587, Burundi : 321, Cambodia : 241, Cameroon : 30700, Canada : 52648, Chile : 80884, China : 3085, Costa Rica : 19957, Croatia : 14477, Cyprus : 1821, Czechia : 29543, Denmark : 10572, Djibouti : 194, Ecuador : 3751, Egypt : 92407, El Salvador : 410, Estonia : 1714, Ethiopia : 210, Fiji : 535, Finland : 20554, France : 280780, Georgia : 2363, Germany : 102151, Ghana : 11894, Greece : 11055, Guatemala : 10504, Haiti : 271, Honduras : 836, Hong Kong : 11358, Hungary : 5828, Iceland : 597, India : 45988, Indonesia : 2588, Iran : 49031, Ireland : 13172, Israel : 38743, Italy : 237300, Jamaica : 3725, Japan : 6473, Jordan : 24301, Kazakhstan : 19526, Lebanon : 17828, Libya : 23357, Lithuania : 5496, Luxembourg : 3548, Macao : 3120, Malaysia : 96482, Maldives : 1443, Malta : 1086, Mauritius : 13127, Mexico : 95413, Moldova : 838, Morocco : 186111, Namibia : 8378, Netherlands : 63486, Nigeria : 79435, Norway : 10672, Palestine : 32102, Panama : 10031, Peru : 112139, Philippines : 19838, Poland : 48518, Portugal : 12324, Puerto Rico : 1366, Qatar : 31542, Russia : 46732, Serbia : 3091, Singapore : 27920, Slovenia : 5756, South Africa : 136118, South Korea : 435, Spain : 134334, Sudan : 25557, Sweden : 16396, Switzerland : 28585, Syria : 36136, Taiwan : 4327, Tunisia : 45933, Turkey : 104798, Turkmenistan : 199, United Arab Emirates : 65306, United Kingdom : 91439, United States : 253462, Uruguay : 17299, Yemen : 11373, \n",
      "Over. Length of file :  3556056\n"
     ]
    }
   ],
   "source": [
    "files = glob(\"../data/surnames/fb_surnames/*.csv\")\n",
    "files.sort()\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for file in files:\n",
    "    \n",
    "    country = file.replace(\"../data/surnames/fb_surnames/\",\"\").replace(\"_surnames.csv\",\"\")\n",
    "    \n",
    "    \n",
    "    df = pd.read_csv(file,skiprows=1,header=None)\n",
    "    df.columns = [\"name\",\"count\"]\n",
    "    latin = df[\"name\"].apply(islatin) #removing names from another alphabet\n",
    "    df = df[latin != 0]\n",
    "    \n",
    "    df[\"name\"] = df[\"name\"].apply(name_cleaner).str.upper()\n",
    "    df = df[df[\"name\"] != \"ERROR\"]\n",
    "    df = df.groupby(\"name\",as_index=False)[\"count\"].sum()\n",
    "    if len(df) < 5000:\n",
    "        df = df[df[\"count\"] >= 2]\n",
    "    else:\n",
    "        df = df[df[\"count\"] >= 7] \n",
    "    len_df = len(df)\n",
    "    \n",
    "    print(country,\":\",len_df,end= \", \")\n",
    "\n",
    "    \n",
    "    df[\"country\"] = country\n",
    "    df_list.append(df)\n",
    "    \n",
    "df = pd.concat(df_list)\n",
    "\n",
    "df = df[~df[\"name\"].str.contains(\".|,|?\",regex=False)]\n",
    "\n",
    "df = df[df[\"name\"].str.len() > 1] #removing one characters names\n",
    "\n",
    "df = df.groupby([\"country\",\"name\"],as_index=False)[\"count\"].sum().dropna()\n",
    "\n",
    "df.to_csv(\"../data/surnames/fb_surnames.csv.zip\",compression=\"zip\",index=False)\n",
    "print(\"\")\n",
    "print(\"Over. Length of file : \", len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63663cde-22c1-4094-b9cb-65d20480fdb9",
   "metadata": {},
   "source": [
    "# Improving the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fb3ad1-da5f-47f7-9856-3d6026959170",
   "metadata": {},
   "source": [
    "The dataset is particularly dirty : Not only do people put fake names and fake locations but also surnames of people from a country don't always originate from that country because of colonialism, migrations or the country being a composite of several ethnicities in the first place.\n",
    "\n",
    "For example, a lot of names from Haiti are french, from Angola are portuguese and from Philippines are spanish. The US dataset contains a lot of spanish names and the french dataset a lot of african and arabic names. Countries from the arabic peninsula have extremely high rates of immigration from the Indian subcontinent so much that a majority of surnames from their dataset isn't arabic. Belgium surnames are either french or dutch and swiss surnames either french, germanic or italian.\n",
    "\n",
    "I follow the approach of MaziÃ¨re and Roth and keep only the names that are highly concentrated in one country. I then add the names from their dataset for countries missing in the facebook data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b94d7db-0438-4c20-9271-92d498bfd3f2",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "306e52aa-024a-400b-8c57-32a8cd65dafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataframe :  3556056\n",
      "Italy done\n",
      "Iran done\n",
      "Countries removed\n",
      "Length of dataframe :  2848834\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/surnames/fb_surnames.csv.zip\",dtype={\"country\" : \"category\", \"name\" : 'string', \"count\" : 'int32'})\n",
    "print(\"Length of dataframe : \", len(df))\n",
    "\n",
    "#lots of italian names in the french data that can be easily removed.\n",
    "italian_end = \"INI ANI ONI TTI OLI LLI GHI INO ERA ERO\"\n",
    "df.loc[(df[\"country\"] == \"France\") & (df[\"name\"].str[-3:].isin(italian_end.split())),\"country\"] = \"Italy\"\n",
    "\n",
    "#Lots of wrong ethnicities in the Italian dataset for some reason, slavic in particular, cleaning it.\n",
    "df_italy = df[df[\"country\"]== \"Italy\"]\n",
    "df_other = df[df[\"country\"] != \"Italy\"]\n",
    "\n",
    "mask = (df_italy[\"count\"] > 10) & (df_italy[\"name\"].str[-1:].isin([\"A\",\"E\",\"I\",\"O\",'N'])) & (~df_italy[\"name\"].str.contains('[YWKXJ]'))\n",
    "df_italy = df_italy[mask]\n",
    "df = pd.concat([df_other,df_italy])\n",
    "print(\"Italy done\")\n",
    "\n",
    "#Polish names were scattered across the world. Making the assumption that the names in SKI and SKY are polish which is mostly right.\n",
    "df.loc[df[\"name\"].str[-3:].isin([\"SKI\",'SKY']),\"country\"] = \"Poland\"\n",
    "\n",
    "#The Iranian data was mixed with a lot of slavic data \n",
    "df_iran = df[df[\"country\"].str.lower() == \"Iran\"]\n",
    "df_other = df[df[\"country\"].str.lower() != \"Iran\"]\n",
    "\n",
    "consonants = \"[KLMCNDPKRSTWVSXYZ]\"\n",
    "mask = (df_iran[\"name\"].str.len() > 5) & (~df_iran['name'].isin(df_other[\"name\"])) & (~df_iran[\"name\"].str.contains(consonants*3 + \"|W|Z[BCDFGKLMNPKRSTWXYZ]|K$|Y$|OS$|J$|O$|A$|ZK|ZV|OV$|VL|ORF$|CHA$|SV|OVA$|SKI|SKY|EK$|SKA|VA$|IK$|EC$|KA$|NA$|KY$|C$|TA$|AN$|ER$\"))\n",
    "df_iran = df_iran[mask]\n",
    "df = pd.concat([df_other,df_iran])\n",
    "print(\"Iran done\")\n",
    "\n",
    "#removing countries that are ambiguous because of a lot of immigration or mixed ethnicities inside them.\n",
    "df= df[~df[\"country\"].isin([\"Switzerland\",\"Belgium\",'United States','Maldives','Philippines','Canada','Israel','Palestine','Papua New Guinea','Luxembourg','Fiji','Qatar','United Arab Emirates','Bahrain'])]\n",
    "print(\"Countries removed\")\n",
    "\n",
    "df.to_csv(\"intermediate_surnames.csv.zip\",compression=\"zip\",index=None)\n",
    "print('Length of dataframe : ', len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0b9a6a-b2d8-4553-beef-0a95abc0724f",
   "metadata": {},
   "source": [
    "## Filtering the names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a411d4e-692f-4231-9d23-28671eba4634",
   "metadata": {},
   "source": [
    "I modify the MaziÃ¨re Roth approach by combining the Herfindahl-Hirschman Index with the Gini index to determine the threshold below which to remove names. \n",
    "\n",
    "Using only the hhi causes the most common names to be removed from the dataset. The gini index puts more emphasis on the upper tail of the distribution. I make the assumption that if the top 2~3 countries have a much higher prevalence of a name than the rest then the name originates from those countries. I find that using the mean of the hhi and gini yields the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e01cbf7e-b597-4744-ad44-feedf56eeee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhi done\n",
      "gini done\n",
      "West Africa done\n",
      "Done. Length of dataframe :  1359594\n"
     ]
    }
   ],
   "source": [
    "def gini_coefficient(x):\n",
    "    diffsum = 0\n",
    "    for i, xi in enumerate(x[:-1], 1):\n",
    "        diffsum += np.sum(np.abs(xi - x[i:]))\n",
    "    return diffsum / (len(x)**2 * np.mean(x))\n",
    "\n",
    "\n",
    "#computing the country level name frequency, removing names that have a frequency lower than 0.0000005\n",
    "df[\"freq\"] = df[\"count\"]/df.groupby(\"country\")[\"count\"].transform(sum)\n",
    "df = df[df[\"freq\"] > 0.0000005]\n",
    "\n",
    "#computing the hhi\n",
    "standardized_freq = df[\"freq\"]/df.groupby(\"name\")[\"freq\"].transform(sum)\n",
    "df[\"hhi\"] = (standardized_freq**2).groupby(df['name']).transform(sum)\n",
    "print(\"hhi done\")\n",
    "\n",
    "#computing the gini index\n",
    "gini = df.groupby('name')[\"freq\"].apply(np.array).apply(gini_coefficient)\n",
    "df = df.merge(gini.rename(\"gini\"),left_on=\"name\",right_index=True)\n",
    "print(\"gini done\")\n",
    "\n",
    "df[\"mean\"] = df[[\"gini\",\"hhi\"]].mean(axis=1)\n",
    "\n",
    "\n",
    "#for some reason names that are most prevalent in France, Angola and Libya combined are all highly common west african names that would be removed otherwise\n",
    "# reclassifying them in Burkina Fasso\n",
    "a = df.sort_values([\"name\",'freq'],ascending=False).groupby('name').head(5) #keeping the top 5 countries for each name\n",
    "a= a.groupby('name')[\"country\"].apply(np.array).astype(str)\n",
    "african_names = a[(a.astype(str).str.contains(\"France\")) & (a.astype(str).str.contains(\"Angola\")) & (a.astype(str).str.contains(\"Libya\"))].index\n",
    "\n",
    "df.loc[(df[\"name\"].isin(african_names)), [\"country\",\"gini\"]] = [\"Burkina Faso\",0]\n",
    "print(\"West Africa done\")\n",
    "\n",
    "df_unique = df[df[\"gini\"] == 0]\n",
    "df_other= df[(df[\"gini\"] != 0)]\n",
    "\n",
    "df_other = df_other[df_other[\"mean\"] > 0.50] #rather arbitrary threshold that I find works well in practice.\n",
    "\n",
    "df_other = df_other.sort_values([\"name\",\"freq\"],ascending=False).drop_duplicates(\"name\")\n",
    "df = pd.concat([df_unique,df_other])\n",
    "\n",
    "df = df[[\"country\",\"name\"]]\n",
    "df.to_csv(\"ready_data.csv.zip\",index=False,compression=\"zip\")\n",
    "print(\"Done. Length of dataframe : \", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abdb049-5c93-400d-9f66-0743c1fa030d",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619f7d4e-e0b1-4d28-a878-33e90a65c616",
   "metadata": {},
   "source": [
    "## Training on facebook data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b6bb084-551a-48ab-a425-513e8d637040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy score :  0.778\n",
      "Cohen Kappa score :  0.775\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CentralSouthEuropean</th>\n",
       "      <td>0.922684</td>\n",
       "      <td>0.845253</td>\n",
       "      <td>0.882273</td>\n",
       "      <td>66528.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Slavic</th>\n",
       "      <td>0.819367</td>\n",
       "      <td>0.867137</td>\n",
       "      <td>0.842576</td>\n",
       "      <td>15437.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arabian</th>\n",
       "      <td>0.841252</td>\n",
       "      <td>0.827542</td>\n",
       "      <td>0.834341</td>\n",
       "      <td>40375.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.833130</td>\n",
       "      <td>0.820285</td>\n",
       "      <td>0.825196</td>\n",
       "      <td>203940.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NorthEuropean</th>\n",
       "      <td>0.804873</td>\n",
       "      <td>0.840544</td>\n",
       "      <td>0.822322</td>\n",
       "      <td>37929.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.820285</td>\n",
       "      <td>0.820285</td>\n",
       "      <td>0.820285</td>\n",
       "      <td>0.820285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>African</th>\n",
       "      <td>0.796608</td>\n",
       "      <td>0.786190</td>\n",
       "      <td>0.791365</td>\n",
       "      <td>27604.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.719016</td>\n",
       "      <td>0.778038</td>\n",
       "      <td>0.735002</td>\n",
       "      <td>203940.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indian</th>\n",
       "      <td>0.628411</td>\n",
       "      <td>0.671129</td>\n",
       "      <td>0.649068</td>\n",
       "      <td>14343.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asian</th>\n",
       "      <td>0.219916</td>\n",
       "      <td>0.608469</td>\n",
       "      <td>0.323067</td>\n",
       "      <td>1724.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      precision    recall  f1-score        support\n",
       "CentralSouthEuropean   0.922684  0.845253  0.882273   66528.000000\n",
       "Slavic                 0.819367  0.867137  0.842576   15437.000000\n",
       "Arabian                0.841252  0.827542  0.834341   40375.000000\n",
       "weighted avg           0.833130  0.820285  0.825196  203940.000000\n",
       "NorthEuropean          0.804873  0.840544  0.822322   37929.000000\n",
       "accuracy               0.820285  0.820285  0.820285       0.820285\n",
       "African                0.796608  0.786190  0.791365   27604.000000\n",
       "macro avg              0.719016  0.778038  0.735002  203940.000000\n",
       "Indian                 0.628411  0.671129  0.649068   14343.000000\n",
       "Asian                  0.219916  0.608469  0.323067    1724.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"ready_data.csv.zip\",dtype={\"country\" : 'category', 'name' : \"string\"})\n",
    "labels = pd.read_csv(\"../data/other_data/labels2.csv\",dtype={\"country\" : 'category','region' : 'category'})\n",
    "df = df.merge(labels[[\"country\",'region']]).dropna()\n",
    "\n",
    "\n",
    "def padding(name):\n",
    "    padded_text = \"^\" + name + \"$\"\n",
    "    return padded_text\n",
    "\n",
    "padder = FunctionTransformer(padding)\n",
    "vectorizer = CountVectorizer(analyzer=\"char_wb\", ngram_range=(2,8))\n",
    "SGD_model = SGDClassifier(class_weight=\"balanced\")\n",
    "\n",
    "\n",
    "FB_clf = Pipeline(steps=[(\"padding\",padder),\n",
    "                      ('vectorizer', vectorizer),\n",
    "                      ('model', SGD_model)])\n",
    "\n",
    "X = df[\"name\"]\n",
    "y = df[\"region\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "\n",
    "FB_clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred = FB_clf.predict(X_test)\n",
    "print_metrics(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1295a077-e143-4038-97ad-b1a4869c0e08",
   "metadata": {},
   "source": [
    "## Training on mixed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489e9801-47ff-4bca-b6bd-1bb354ceba24",
   "metadata": {},
   "source": [
    "Let's add the data from the Pubmed dataset for Asian and Indian regions where the facebook data is lacking and performance is poor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "de5d1d61-8b5f-43bc-880d-213940b84166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy score :  0.814\n",
      "Cohen Kappa score :  0.779\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CentralSouthEuropean</th>\n",
       "      <td>0.916199</td>\n",
       "      <td>0.845268</td>\n",
       "      <td>0.879305</td>\n",
       "      <td>66431.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Slavic</th>\n",
       "      <td>0.806850</td>\n",
       "      <td>0.872742</td>\n",
       "      <td>0.838503</td>\n",
       "      <td>15331.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arabian</th>\n",
       "      <td>0.826987</td>\n",
       "      <td>0.818468</td>\n",
       "      <td>0.822705</td>\n",
       "      <td>40676.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.822411</td>\n",
       "      <td>0.818507</td>\n",
       "      <td>0.819368</td>\n",
       "      <td>223860.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.818507</td>\n",
       "      <td>0.818507</td>\n",
       "      <td>0.818507</td>\n",
       "      <td>0.818507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NorthEuropean</th>\n",
       "      <td>0.790631</td>\n",
       "      <td>0.843648</td>\n",
       "      <td>0.816279</td>\n",
       "      <td>37729.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.790744</td>\n",
       "      <td>0.813649</td>\n",
       "      <td>0.800615</td>\n",
       "      <td>223860.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>African</th>\n",
       "      <td>0.766605</td>\n",
       "      <td>0.788021</td>\n",
       "      <td>0.777166</td>\n",
       "      <td>27564.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Asian</th>\n",
       "      <td>0.677766</td>\n",
       "      <td>0.819521</td>\n",
       "      <td>0.741933</td>\n",
       "      <td>11475.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Indian</th>\n",
       "      <td>0.750172</td>\n",
       "      <td>0.707877</td>\n",
       "      <td>0.728411</td>\n",
       "      <td>24654.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      precision    recall  f1-score        support\n",
       "CentralSouthEuropean   0.916199  0.845268  0.879305   66431.000000\n",
       "Slavic                 0.806850  0.872742  0.838503   15331.000000\n",
       "Arabian                0.826987  0.818468  0.822705   40676.000000\n",
       "weighted avg           0.822411  0.818507  0.819368  223860.000000\n",
       "accuracy               0.818507  0.818507  0.818507       0.818507\n",
       "NorthEuropean          0.790631  0.843648  0.816279   37729.000000\n",
       "macro avg              0.790744  0.813649  0.800615  223860.000000\n",
       "African                0.766605  0.788021  0.777166   27564.000000\n",
       "Asian                  0.677766  0.819521  0.741933   11475.000000\n",
       "Indian                 0.750172  0.707877  0.728411   24654.000000"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"ready_data.csv.zip\",dtype={\"country\" : 'category', 'name' : \"string\"})\n",
    "labels = pd.read_csv(\"../data/other_data/labels2.csv\",dtype={\"country\" : 'category','region' : 'category'})\n",
    "df = df.merge(labels[[\"country\",'region']])\n",
    "\n",
    "#Lets oversample east asian countries which have both about a couple hundred extremely common names so that they don't get drowned in the data\n",
    "df_east_asia = resample(df[df[\"country\"].isin([\"China\",'South Korea',\"Taiwan\",'Macao'])], n_samples=20000)\n",
    "\n",
    "\n",
    "df_maz = pd.read_csv('../data/other_data/maziere_roth_dataset.csv' ,dtype={\"country\" : 'category', 'name' : \"string\"})\n",
    "df_maz = df_maz[df_maz[\"region\"].isin(['Asian','Indian'])]\n",
    "\n",
    "df = pd.concat([df,df_maz,df_east_asia]).dropna()\n",
    "\n",
    "padder = FunctionTransformer(padding)\n",
    "vectorizer = CountVectorizer(analyzer=\"char_wb\", ngram_range=(2,8))\n",
    "SGD_model = SGDClassifier(class_weight=\"balanced\")\n",
    "\n",
    "MX_clf = Pipeline(steps=[(\"padding\",padder),\n",
    "                      ('vectorizer', vectorizer),\n",
    "                      ('model', SGD_model)])\n",
    "\n",
    "X = df[\"name\"]\n",
    "y = df[\"region\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "\n",
    "MX_clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred = MX_clf.predict(X_test)\n",
    "print_metrics(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7d8bc7f0-4dc1-4ca9-afb9-155a79bf3456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mixed_model.joblib']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saving the model\n",
    "from joblib import dump\n",
    "dump(MX_clf, 'mixed_model.joblib') \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_data",
   "language": "python",
   "name": "base_data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
